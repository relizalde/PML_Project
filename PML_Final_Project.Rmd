Predicting Quality of Weight Lifting Exercises
====================================

Introduction
--------------

This project is part of the __Practical Machine Learning__ course given at Coursera by _Johns Hopkins University_. The main objective is to predict how well an activity was performed by a person using an on-body sensor. 

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions (classes).  One class represents performing the exercise correctly, the remaining four classes represent common mistakes.

The approach taken in this work was using _Random Forests_. The following sections describe the data preparation, the model building and validation.

Data Preparation
--------------

### Loading Data

After downloading the datasets, we start working with the data:

```{r, eval=FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv") 
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")
```

Reading the files:
```{r}
raw.training <- read.csv("pml-training.csv",stringsAsFactors=FALSE,na.strings=c("NA",""))
raw.testing <- read.csv("pml-testing.csv",stringsAsFactors=FALSE,na.strings=c("NA",""))
```

### Cleaning Data

Many attributes in datasets have a lot of missing values so we decided to remove all columns that contain mostly NAs in the training set and do the same for the testing set.
```{r}
NAs <- apply(raw.training,2,function(x) {sum(is.na(x))}) 
training <- raw.training[,which(NAs == 0)]
testing <- raw.testing[,which(NAs == 0)]
```

Also, we remove the columns with non-numeric variables such as `user_name`, time stamps and window (Columns 1..6). These attributes are not generalizable for other data sets.
```{r}
rem.Idx <- as.integer(c(1,2,3,4,5,6,7))

training <- training[,-rem.Idx]
training$classe <- as.factor(training$classe)

testing <- testing[,-rem.Idx]
```

Model Building
--------------

For this section we are going to use the `caret` package in R.
```{r ,warning=FALSE,message=FALSE}
library(caret)
```

First step is to partition the training data for considering cross-validation. The training data set was split up into one portion (70%) for model building and another portion (30%) for cross-validation.
```{r}
set.seed(1234)
inTrain = createDataPartition(training$classe, p = 0.7, list=FALSE)
trainSet = training[ inTrain,]
cv.testSet = training[-inTrain,]
```

I decided to try out _Random Forests_ as the prediction algorithm. The model was train on the `trainSet` and then cross-validated on the `cv.testSet`.

### Random Forests

We build the random forest model with a 5-fold cross validation. We run the `train` function with default tuning parameters and find out that the optimal `mtry` parameter for this case is **27**. 
```{r,warning=FALSE,message=FALSE}
library(randomForest)
```

```{r}
set.seed(7513)

mControl <- trainControl(method = "cv", number=5)
modelFit <- train(classe ~ ., data = trainSet, method="rf", trControl = mControl)
modelFit
```

#### *Cross Validation*

Now we proceed with the predictions for the cross validation.

```{r}
cv.pred <- predict(modelFit, cv.testSet)
```

We use a _Confusion Matrix_ in order to evaluate the accuracy of our model. Each column of the matrix represents the instances in a predicted class, while each row represents the instances in an actual (reference) class.

```{r}
confMatrix <- confusionMatrix(cv.pred, cv.testSet$classe)
confMatrix$table
```

The accuracy for the random forest model is 99.4%.
```{r}
confMatrix$overall["Accuracy"]
```

For cross validation, a 10-fold cross validation was performed. The training data was randomly splitted into 10 parts, and each of the 10 parts was the testing set, and the other 9 parts were the training set. After the loop runs 10 times, we will get the average accuracy on how the model performed on the testing sets. Based on the result based, the average accuracy was 0.999 and the average kappa was 0.998 , which are fairly high.

```{r}
# 10-fold cross validation
set.seed(123)
k=10
parts <- split(training,f = rep_len(1:k, nrow(training) ))

# make a function to combine the list of 10 equal size data
combinedata <- function(index){
  data <- parts[[index[1]]]
  for (i in 2:(length(index))) data <- rbind(data, parts[[index[i]]])
  data
}

# set empty matrix to store result
cross_validation_result <- as.data.frame(matrix(nrow=7, ncol=k))

index <- 1:10

for (i in 1:10){
  currentdata <- combinedata(index[index!= i])
  model <- randomForest(classe~., data=currentdata)
  result <- confusionMatrix(parts[[i]]$classe, predict(model, newdata=parts[[i]]))
  cross_validation_result[,i] <- result$overall
}
```

Expected Out of Sample Error
--------------
The out of sample error is just the error rate that we get when we apply the classification model on a new data set. Therefore, it was just the error rate from the 10-fold cross validation samples. We found that the errors were 0.003, 0.003, 0.003, 0.002, 0.002, 0.004, 0.001, 0.003, 0.004, 0.003. Taking an average of those errors, we got an out of sample error rate of 0.003. Therefore, we can expect the out of sample error for other testing sets to be __0.3%__.

Applying the Model
--------------

Now that the Random Forests model was build and tested, we proceed to apply this model in order to predict the `classe` on the testing set provided.
```{r}
testing.pred <- predict(modelFit, testing)
```

```{r, echo=FALSE}
cat("Predicted 'classe' values for the 20 cases on the testing set\n")
testing.pred
```

We could predict the 20 test cases provided with 100% accuracy using this model.

References
--------------

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: [Groupware@LES: Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har)